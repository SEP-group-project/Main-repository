{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from torchvision import models , transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12dfd78",
   "metadata": {},
   "source": [
    "#from data_import import train_images, test_images\n",
    "from xAI.occlusion import occlusion_saliency\n",
    "from xAI.gradcam import gradcam, overlay_heatmap\n",
    "from xAI.LayerActivation import get_layer_activation\n",
    "from xAI.smoothGrad import coumpute_smoothGrad\n",
    "from classification_model import EmotionCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        def block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout2d(0.15),\n",
    "            )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            block(3, 32),\n",
    "            block(32, 64),\n",
    "            block(64, 128),\n",
    "            block(128, 256),\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448784a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_last_conv_layer(model):\n",
    "    \n",
    "    return model.features[3][3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradcam(model, face_bgr, class_idx):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, (64, 64))\n",
    "    x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "    \n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "\n",
    "    conv_layer = get_last_conv_layer(model)\n",
    "    \n",
    "\n",
    "    activations = None\n",
    "    gradients = None\n",
    "    \n",
    "    def forward_hook(module, input, output): \n",
    "        nonlocal activations\n",
    "        activations = output     #output: feature maps of the last conv-layer\n",
    "    \n",
    "    def backward_hook(module, grad_in, grad_out): \n",
    "        nonlocal gradients\n",
    "        gradients = grad_out[0]  #gradient of feature maps\n",
    "\n",
    "    \n",
    "    hook_f = conv_layer.register_forward_hook(forward_hook)  \n",
    "    hook_b = conv_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    \n",
    "    preds = model(x)\n",
    "    score = preds[:, class_idx]  # score of the predicted class\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    score.backward()\n",
    "\n",
    "    hook_b.remove()\n",
    "    hook_f.remove()\n",
    "\n",
    "    gradients = gradients.detach().cpu().numpy()[0]\n",
    "    activations = activations.detach().cpu().numpy()[0]\n",
    "\n",
    "    weights = np.mean(gradients, axis=(1,2)) # importance of each feature map \n",
    "                                             # gradient.shape: (C, H, W)\n",
    "                                             # weights stores one weight per feature map\n",
    "\n",
    "    \n",
    "    heatmap = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights):\n",
    "        heatmap += w*activations[i]   # one weighted feature map\n",
    "    \n",
    "    heatmap = np.maximum(heatmap, 0) # apply ReLU\n",
    "    heatmap /= heatmap.max()+ 1e-8   # normalize\n",
    "\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def overlay_heatmap(img, heatmap, alpha=0.4):\n",
    "    \n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    superimposed_img = cv2.addWeighted(img, alpha, heatmap, 1-alpha, 0)\n",
    "    return superimposed_img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb63e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, NoiseTunnel\n",
    "import cv2\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "def coumpute_smoothGrad(model, img, target_class, samples):\n",
    "\n",
    "    \n",
    "    face_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, (64, 64))\n",
    "    x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency = Saliency(model)\n",
    "    nt = NoiseTunnel(saliency)\n",
    "    attribution = nt.attribute(x, nt_type='smoothgrad',nt_samples=samples, target=target_class )\n",
    "\n",
    "    attr_np = attribution.squeeze().cpu().detach().numpy()\n",
    "    heatmap = np.sum(np.abs(attr_np), axis=0)\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "    \n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerActivation:\n",
    "    def __init__(self, layer: torch.nn.Module):\n",
    "        self.activation = None\n",
    "        self.hook = layer.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.activation = output.detach().cpu()\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def get_layer_activation(model, layer, image_tensor):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    hook = LayerActivation(layer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(image_tensor)\n",
    "\n",
    "    activation = hook.activation\n",
    "    hook.remove()\n",
    "\n",
    "    if activation is None:\n",
    "        raise RuntimeError(\"Hook did not capture any activation, check that the layer is used in forward.\")\n",
    "\n",
    "    return activation\n",
    "\n",
    "def list_conv_layers(model: nn.Module):\n",
    "    convs = []\n",
    "    for name, m in model.named.modules():\n",
    "        is isinstance(m, nn.Conv2d):\n",
    "        convs.append((name,m))\n",
    "    return convs\n",
    "\n",
    "def get_conv_layer(model: nn.Module, which: str = \"last\") -> nn.Module:\n",
    "    convs = list_conv_layers(model)\n",
    "    if not convs:\n",
    "        raise RuntimeError(\"No conv2d layers found in this model.\")\n",
    "\n",
    "    if which == \"first\":\n",
    "        return convs[0][1]\n",
    "    elif which == \"middle\":\n",
    "        return convs[len(convs)//2][1]\n",
    "    elif which == \"last\":\n",
    "        return convs[-1][1]\n",
    "    else:\n",
    "        for name, layer in convs:\n",
    "            if name == which:\n",
    "                return layer\n",
    "        raise ValueError(f\"Unknown layer selector '{which}'. Use 'first'/'middle'/'last' or a conv layer name.\")\n",
    "\n",
    "def layer_activation_heatmap_from_tensor(activation: torch.Tensor) -> torch.Tensor:\n",
    "    if activation.dim() != 4:\n",
    "        raise ValueError( \"Activation must have shape [B, C, H, W]\")\n",
    "\n",
    "    heat = activation.mean(dim=1)[0]\n",
    "    heat = heat-heat.min\n",
    "    heat = heat / (heat.max() + 1e-8)\n",
    "\n",
    "    return heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273b375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def occlusion_saliency(\n",
    "        model,\n",
    "        img,\n",
    "        target_class: int,\n",
    "        patch_size: int = 8,\n",
    "        stride: int = 4,\n",
    "        baseline: float = 0.0,\n",
    "        use_softmax = False,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if img.dim() != 4 or img.shape[0] != 1:\n",
    "        raise ValueError(\"Image must have shape [1, C, H, W] \")\n",
    "\n",
    "    device = img.device\n",
    "    _, C, H, W = img.shape\n",
    "\n",
    "    out = model(img)\n",
    "    if use_softmax:\n",
    "        out = torch.softmax(out, dim=1)\n",
    "    base_score = out[0, target_class].item()\n",
    "\n",
    "    sal_sum = torch.zeros((H,W), device=device)\n",
    "    sal_cnt = torch.zeros((H,W), device=device)\n",
    "\n",
    "    for y in range(0, H, stride):\n",
    "        y1 = y\n",
    "        y2 = min(y + patch_size, H)\n",
    "\n",
    "        for x in range(0, W, stride):\n",
    "            x1 = x\n",
    "            x2 = min(x + patch_size, W)\n",
    "\n",
    "            occ = img.clone()\n",
    "            occ[:, :, y1:y2, x1:x2] = baseline\n",
    "\n",
    "            out_occ = model(occ)\n",
    "            if use_softmax:\n",
    "                out_occ = torch.softmax(out_occ, dim=1)\n",
    "            \n",
    "            occ_score = out_occ[0, target_class].item()\n",
    "            drop = base_score - occ_score\n",
    "            \n",
    "            sal_sum[y1:y2, x1:x2] += drop\n",
    "            sal_cnt[y1:y2, x1:x2] += 1.0\n",
    "\n",
    "    sal = sal_sum / torch.clamp(sal_cnt, min=1.0)\n",
    "    sal = torch.clamp(sal, min=0.0)\n",
    "\n",
    "    s_min, s_max = sal.min(), sal.max()\n",
    "    if (s_max > s_min) > 1e-12:\n",
    "        sal = (sal - s_min) / (s_max - s_min)\n",
    "    else:\n",
    "        sal = torch.zeros_like(sal)\n",
    "\n",
    "    return sal.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_emotion = {\n",
    "    0: \"surprise\",    \n",
    "    1: \"fear\",        \n",
    "    2: \"disgust\",     \n",
    "    3: \"happiness\",   \n",
    "    4: \"sadness\",     \n",
    "    5: \"anger\",       \n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "model = EmotionCNN(num_classes=6).to(device)\n",
    "WEIGHTS_PATH = \"best_model_cosine.pt\"\n",
    "state = torch.load(WEIGHTS_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_emotion(face_bgr):\n",
    "    with torch.no_grad():\n",
    "     face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "     face_rgb = cv2.resize(face_rgb, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "     x = preprocess(face_rgb).unsqueeze(0).to(device)  \n",
    "     logits = model(x)\n",
    "     probs = torch.softmax(logits, dim=1)\n",
    "     conf, pred = torch.max(probs, dim=1)\n",
    "\n",
    "     pred_idx = int(pred.item())\n",
    "     conf = float(conf.item())\n",
    "     emotion = idx_to_emotion.get(pred_idx, str(pred_idx))\n",
    "     return emotion, conf, pred_idx\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "face_detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "if face_detector.empty():\n",
    "    raise RuntimeError(\"couldn't load haarcascade_frontalface_default.xml \") #change:i put the face detector outside the loop cuz its more efficient this way\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"couldn't open webcam.\")\n",
    "\n",
    "\n",
    "MODE = \"none\"  # \"none\", \"gradcam\", \"vanilla\", \"occlusion\", \"smoothGrad\", \"activation\"\n",
    "FROZEN_FRAME = None\n",
    "\n",
    "\n",
    "while True:\n",
    "    if MODE == \"none\":\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "    \n",
    "      frame = cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    else:\n",
    "       frame = FROZEN_FRAME.copy()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        pad = int(0.15 * w)\n",
    "        x1 = max(0, x - pad)\n",
    "        y1 = max(0, y - pad)\n",
    "        x2 = min(frame.shape[1], x + w + pad)\n",
    "        y2 = min(frame.shape[0], y + h + pad)\n",
    "\n",
    "        face_roi = frame[y1:y2, x1:x2]\n",
    "        emotion, conf, pred_idx = predict_emotion(face_roi)\n",
    "\n",
    "        if MODE == \"gradcam\":\n",
    "           heatmap = gradcam(model,face_roi,pred_idx)\n",
    "           superimposed_img = overlay_heatmap(face_roi,heatmap)\n",
    "\n",
    "        elif MODE == \"vanilla\":\n",
    "           heatmap = vanilla_grad_saliency(model, face_roi, pred_idx)\n",
    "           superimposed_img = overlay_heatmap(face_roi, heatmap)\n",
    "\n",
    "        elif MODE == \"occlusion\":\n",
    "           heatmap = occlusion_saliency(model, face_roi, pred_idx)\n",
    "           superimposed_img = overlay_heatmap(face_roi, heatmap)\n",
    "\n",
    "           frame[y1:y2, x1:x2] = superimposed_img\n",
    "        \n",
    "        elif MODE ==\"smoothgrad\":\n",
    "            heatmap = coumpute_smoothGrad(model, face_roi, pred_idx, 20)\n",
    "            superimposed_img = overlay_heatmap(face_roi, heatmap)\n",
    "\n",
    "        elif MODE == \"activation\":\n",
    "            heatmap = get_layer_activation(model, face_roi, which_layer=\"last\")\n",
    "            superimposed_img = gradcam.overlay_heatmap(face_roi, heatmap)\n",
    "\n",
    "        if MODE in [\"gradcam\", \"vanilla\", \"occlusion\", \"smoothgrad\", \"activation\"]:\n",
    "            frame[y1:y2, x1:x2] = superimposed_img\n",
    "\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        label = f\"{emotion}: {conf:.2f} | {MODE}\" \n",
    "        cv2.putText(frame, label, (x1, max(20, y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "    cv2.imshow('emotion detector', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('g'):\n",
    "       MODE = \"none\" if MODE == \"gradcam\" else \"gradcam\"\n",
    "       FROZEN_FRAME = frame.copy() if MODE != \"none\" else None\n",
    "\n",
    "    if key == ord('v'):\n",
    "       MODE = \"none\" if MODE == \"vanilla\" else \"vanilla\"\n",
    "       FROZEN_FRAME = frame.copy() if MODE != \"none\" else None\n",
    "\n",
    "    if key == ord('o'):\n",
    "       MODE = \"none\" if MODE == \"occlusion\" else \"occlusion\"\n",
    "       FROZEN_FRAME = frame.copy() if MODE != \"none\" else None\n",
    "\n",
    "    if key == ord('s'):\n",
    "        MODE = \"none\" if MODE == \"smoothgrad\" else \"smoothgrad\"\n",
    "        FROZEN_FRAME = frame.copy() if MODE != \"none\" else None\n",
    "\n",
    "    if key == ord('a'):\n",
    "        MODE = \"none\" if MODE == \"activation\" else \"activation\"\n",
    "        FROZEN_FRAME = frame.copy() if MODE != \"none\" else None\n",
    "\n",
    "    if key == ord('n'):\n",
    "       MODE = \"none\"\n",
    "       FROZEN_FRAME = None\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break  \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f16580",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "idx_to_emotion = {\n",
    "    0: \"surprise\", 1: \"fear\", 2: \"disgust\",\n",
    "    3: \"happiness\", 4: \"sadness\", 5: \"anger\",\n",
    "}\n",
    "\n",
    "\n",
    "model = EmotionCNN(num_classes=6).to(device)\n",
    "state = torch.load(\"best_model_cosine.pt\", map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "face_detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "if face_detector.empty():\n",
    "    raise RuntimeError(\"Could not load haarcascade_frontalface_default.xml\")\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "\n",
    "def predict_emotion(face_bgr):\n",
    "    with torch.no_grad():\n",
    "        face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "        face_rgb = cv2.resize(face_rgb, (64,64))\n",
    "        x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        conf, pred = torch.max(probs, dim=1)\n",
    "        pred_idx = int(pred.item())\n",
    "        conf = float(conf.item())\n",
    "        emotion = idx_to_emotion.get(pred_idx, str(pred_idx))\n",
    "        return emotion, conf, pred_idx\n",
    "\n",
    "def compute_xai_overlay(face_bgr, mode):\n",
    "    \"\"\"Compute heatmap overlay for any XAI mode\"\"\"\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, (64,64))\n",
    "    x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    # Predict\n",
    "    emotion, conf, pred_idx = predict_emotion(face_bgr)\n",
    "\n",
    "    # Compute heatmap\n",
    "    try:\n",
    "        if mode == \"gradcam\":\n",
    "            heatmap = gradcam(model, face_bgr, pred_idx)\n",
    "        elif mode == \"smoothgrad\":\n",
    "            heatmap = coumpute_smoothGrad(model, face_bgr, pred_idx, samples=50)\n",
    "        elif mode == \"vanilla\":\n",
    "            heatmap = vanilla_grad_saliency(model, face_bgr, pred_idx)\n",
    "        elif mode == \"occlusion\":\n",
    "            heatmap = occlusion_saliency(model, face_bgr, pred_idx)\n",
    "        elif mode == \"activation\":\n",
    "            heatmap = get_layer_activation(model, face_bgr, which_layer=\"last\")\n",
    "    except Exception as e:\n",
    "        print(\"XAI computation error:\", e)\n",
    "        heatmap = np.zeros((64,64), dtype=np.float32)\n",
    "\n",
    "    # Normalize heatmap properly\n",
    "    heatmap = np.abs(heatmap)\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "\n",
    "    overlay_img = overlay_heatmap(face_bgr, heatmap)\n",
    "    return overlay_img, emotion, conf\n",
    "\n",
    "# --- Live demo loop ---\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (640,480))\n",
    "    display_frame = frame.copy()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw rectangles and labels for live frame\n",
    "    for (x, y, w, h) in faces:\n",
    "        pad = int(0.15*w)\n",
    "        x1, y1 = max(0,x-pad), max(0,y-pad)\n",
    "        x2, y2 = min(frame.shape[1], x+w+pad), min(frame.shape[0], y+h+pad)\n",
    "        face_roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "        emotion, conf, pred_idx = predict_emotion(face_roi)\n",
    "        cv2.rectangle(display_frame, (x1,y1), (x2,y2), (0,255,0),2)\n",
    "        label = f\"{emotion}: {conf:.2f}\"\n",
    "        cv2.putText(display_frame, label, (x1,max(20,y1-10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,0),2,cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Live Emotion Detector\", display_frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # --- XAI overlay keys ---\n",
    "    if key in [ord('g'), ord('s'), ord('v'), ord('o'), ord('a')]:\n",
    "        mode = {ord('g'):\"gradcam\", ord('s'):\"smoothgrad\",\n",
    "                ord('v'):\"vanilla\", ord('o'):\"occlusion\",\n",
    "                ord('a'):\"activation\"}[key]\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            x, y, w, h = faces[0]  # first face\n",
    "            pad = int(0.15*w)\n",
    "            x1, y1 = max(0,x-pad), max(0,y-pad)\n",
    "            x2, y2 = min(frame.shape[1], x+w+pad), min(frame.shape[0], y+h+pad)\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "            overlay_img, emotion, conf = compute_xai_overlay(face_roi, mode)\n",
    "            window_name = f\"{mode.upper()} Overlay\"\n",
    "            cv2.imshow(window_name, overlay_img)\n",
    "        else:\n",
    "            print(\"No face detected for XAI overlay.\")\n",
    "\n",
    "    # --- Close overlay windows ---\n",
    "    if key == ord('n'):\n",
    "        for win in [\"GRADCAM Overlay\",\"SMOOTHGRAD Overlay\",\"VANILLA Overlay\",\"OCCLUSION Overlay\",\"ACTIVATION Overlay\"]:\n",
    "            cv2.destroyWindow(win)\n",
    "\n",
    "    # --- Quit ---\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
