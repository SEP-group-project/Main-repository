{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from torchvision import models , transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        def block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout2d(0.15),\n",
    "            )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            block(3, 32),\n",
    "            block(32, 64),\n",
    "            block(64, 128),\n",
    "            block(128, 256),\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448784a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_last_conv_layer(model):\n",
    "    \n",
    "    return model.features[3][3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradcam(model, face_bgr, class_idx):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, (64, 64))\n",
    "    x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "    \n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "\n",
    "    conv_layer = get_last_conv_layer(model)\n",
    "    \n",
    "\n",
    "    activations = None\n",
    "    gradients = None\n",
    "    \n",
    "    def forward_hook(module, input, output): \n",
    "        nonlocal activations\n",
    "        activations = output     #output: feature maps of the last conv-layer\n",
    "    \n",
    "    def backward_hook(module, grad_in, grad_out): \n",
    "        nonlocal gradients\n",
    "        gradients = grad_out[0]  #gradient of feature maps\n",
    "\n",
    "    \n",
    "    hook_f = conv_layer.register_forward_hook(forward_hook)  \n",
    "    hook_b = conv_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    \n",
    "    preds = model(x)\n",
    "    score = preds[:, class_idx]  # score of the predicted class\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    score.backward()\n",
    "\n",
    "    hook_b.remove()\n",
    "    hook_f.remove()\n",
    "\n",
    "    gradients = gradients.detach().cpu().numpy()[0]\n",
    "    activations = activations.detach().cpu().numpy()[0]\n",
    "\n",
    "    weights = np.mean(gradients, axis=(1,2)) # importance of each feature map \n",
    "                                             # gradient.shape: (C, H, W)\n",
    "                                             # weights stores one weight per feature map\n",
    "\n",
    "    \n",
    "    heatmap = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights):\n",
    "        heatmap += w*activations[i]   # one weighted feature map\n",
    "    \n",
    "    heatmap = np.maximum(heatmap, 0) # apply ReLU\n",
    "    heatmap /= heatmap.max()+ 1e-8   # normalize\n",
    "\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def overlay_heatmap(img, heatmap, alpha=0.4):\n",
    "    \n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    superimposed_img = cv2.addWeighted(img, alpha, heatmap, 1-alpha, 0)\n",
    "    return superimposed_img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb63e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, NoiseTunnel\n",
    "import cv2\n",
    "import numpy\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "def coumpute_smoothGrad(model, img, target_class, samples):\n",
    "\n",
    "    \n",
    "    face_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, (64, 64))\n",
    "    x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saliency = Saliency(model)\n",
    "    nt = NoiseTunnel(saliency)\n",
    "    attribution = nt.attribute(x, nt_type='smoothgrad',nt_samples=samples, target=target_class )\n",
    "\n",
    "    attr_np = attribution.squeeze().cpu().detach().numpy()\n",
    "    heatmap = np.sum(np.abs(attr_np), axis=0)\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "    \n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Hook class\n",
    "# -------------------------\n",
    "class LayerActivation:\n",
    "    def __init__(self, layer: nn.Module):\n",
    "        self.activation = None\n",
    "        self.hook = layer.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.activation = output.detach().cpu()\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "# -------------------------\n",
    "# Get layer activation\n",
    "# -------------------------\n",
    "def get_layer_activation(model, layer, image_tensor):\n",
    "    model.eval()\n",
    "    hook = LayerActivation(layer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(image_tensor)\n",
    "\n",
    "    activation = hook.activation\n",
    "    hook.remove()\n",
    "\n",
    "    if activation is None:\n",
    "        raise RuntimeError(\n",
    "            \"Hook did not capture any activation. \"\n",
    "            \"Check that the layer is used in forward().\"\n",
    "        )\n",
    "\n",
    "    return activation\n",
    "\n",
    "# -------------------------\n",
    "# Conv layer helpers\n",
    "# -------------------------\n",
    "def list_conv_layers(model: nn.Module):\n",
    "    return [\n",
    "        (name, m)\n",
    "        for name, m in model.named_modules()\n",
    "        if isinstance(m, nn.Conv2d)\n",
    "    ]\n",
    "\n",
    "def get_conv_layer(model: nn.Module, which: str = \"last\") -> nn.Module:\n",
    "    convs = list_conv_layers(model)\n",
    "    if not convs:\n",
    "        raise RuntimeError(\"No Conv2d layers found in model.\")\n",
    "\n",
    "    if which == \"first\":\n",
    "        return convs[0][1]\n",
    "    elif which == \"middle\":\n",
    "        return convs[len(convs) // 2][1]\n",
    "    elif which == \"last\":\n",
    "        return convs[-1][1]\n",
    "    else:\n",
    "        for name, layer in convs:\n",
    "            if name == which:\n",
    "                return layer\n",
    "        raise ValueError(\n",
    "            f\"Unknown layer '{which}'. \"\n",
    "            \"Use 'first', 'middle', 'last' or layer name.\"\n",
    "        )\n",
    "\n",
    "# -------------------------\n",
    "# Activation → Heatmap\n",
    "# -------------------------\n",
    "def layer_activation_heatmap_from_tensor(\n",
    "    activation: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    if activation.dim() != 4:\n",
    "        raise ValueError(\"Activation must have shape [B, C, H, W]\")\n",
    "\n",
    "    heat = activation.mean(dim=1)[0]\n",
    "\n",
    "    min_val = heat.min()\n",
    "    max_val = heat.max()\n",
    "\n",
    "    if (max_val - min_val) < 1e-8:\n",
    "        return torch.zeros_like(heat)\n",
    "\n",
    "    heat = (heat - min_val) / (max_val - min_val)\n",
    "    return heat\n",
    "\n",
    "# -------------------------\n",
    "# Overlay heatmap\n",
    "# -------------------------\n",
    "def overlay_heatmap(img, heatmap, alpha=0.4):\n",
    "    \"\"\"\n",
    "    img: BGR image (H, W, 3), uint8\n",
    "    heatmap: (H, W) torch.Tensor or np.ndarray in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    # Torch → NumPy\n",
    "    if isinstance(heatmap, torch.Tensor):\n",
    "        heatmap = heatmap.detach().cpu().numpy()\n",
    "\n",
    "    heatmap = heatmap.astype(np.float32)\n",
    "\n",
    "    heatmap = cv2.resize(\n",
    "        heatmap,\n",
    "        (img.shape[1], img.shape[0])\n",
    "    )\n",
    "\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(\n",
    "        heatmap,\n",
    "        cv2.COLORMAP_JET\n",
    "    )\n",
    "\n",
    "    return cv2.addWeighted(\n",
    "        img, alpha,\n",
    "        heatmap, 1 - alpha,\n",
    "        0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f16580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "idx_to_emotion = {\n",
    "    0: \"surprise\", 1: \"fear\", 2: \"disgust\",\n",
    "    3: \"happiness\", 4: \"sadness\", 5: \"anger\",\n",
    "}\n",
    "\n",
    "\n",
    "model = EmotionCNN(num_classes=6).to(device)\n",
    "state = torch.load(\"best_model_cosine.pt\", map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "face_detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "if face_detector.empty():\n",
    "    raise RuntimeError(\"Could not load haarcascade_frontalface_default.xml\")\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam.\")\n",
    "\n",
    "\n",
    "def predict_emotion(face_bgr):\n",
    "    with torch.no_grad():\n",
    "        face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "        face_rgb = cv2.resize(face_rgb, (64,64))\n",
    "        x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        conf, pred = torch.max(probs, dim=1)\n",
    "        pred_idx = int(pred.item())\n",
    "        conf = float(conf.item())\n",
    "        emotion = idx_to_emotion.get(pred_idx, str(pred_idx))\n",
    "        return emotion, conf, pred_idx\n",
    "\n",
    "def compute_xai_overlay(face_bgr, mode):\n",
    "    \"\"\"Compute heatmap overlay for any XAI mode\"\"\"\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, (64,64))\n",
    "    x = preprocess(face_rgb).unsqueeze(0).to(device)\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    # Predict\n",
    "    emotion, conf, pred_idx = predict_emotion(face_bgr)\n",
    "\n",
    "    # Compute heatmap\n",
    "    try:\n",
    "        if mode == \"gradcam\":\n",
    "            heatmap = gradcam(model, face_bgr, pred_idx)\n",
    "        elif mode == \"smoothgrad\":\n",
    "            heatmap = coumpute_smoothGrad(model, face_bgr, pred_idx, samples=50)\n",
    "        elif mode == \"activation\":\n",
    "            layer = get_conv_layer(model, which=\"last\")\n",
    "            activation = get_layer_activation(model, layer, x)\n",
    "            heatmap = layer_activation_heatmap_from_tensor(activation)\n",
    "    except Exception as e:\n",
    "        print(\"XAI computation error:\", e)\n",
    "        heatmap = np.zeros((64,64), dtype=np.float32)\n",
    "\n",
    "    # Normalize heatmap properly\n",
    "    heatmap = np.abs(heatmap)\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "\n",
    "    overlay_img = overlay_heatmap(face_bgr, heatmap)\n",
    "    text = f\"{emotion}: {conf:.2f}\"\n",
    "    cv2.putText(\n",
    "        overlay_img,\n",
    "        text,\n",
    "        (5, 20),  # top-left corner\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.6,\n",
    "        (255,255,255),  # white text\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "    return overlay_img, emotion, conf\n",
    "\n",
    "# Live demo loop \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (640,480))\n",
    "    display_frame = frame.copy()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw rectangles and labels for live frame\n",
    "    for (x, y, w, h) in faces:\n",
    "        pad = int(0.15*w)\n",
    "        x1, y1 = max(0,x-pad), max(0,y-pad)\n",
    "        x2, y2 = min(frame.shape[1], x+w+pad), min(frame.shape[0], y+h+pad)\n",
    "        face_roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "        emotion, conf, pred_idx = predict_emotion(face_roi)\n",
    "        cv2.rectangle(display_frame, (x1,y1), (x2,y2), (0,255,0),2)\n",
    "        label = f\"{emotion}: {conf:.2f}\"\n",
    "        cv2.putText(display_frame, label, (x1,max(20,y1-10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,0),2,cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Live Emotion Detector\", display_frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # --- XAI overlay keys ---\n",
    "    if key in [ord('g'), ord('s'), ord('v'), ord('a')]:\n",
    "        mode = {ord('g'):\"gradcam\", ord('s'):\"smoothgrad\", ord('a'):\"activation\"}[key]\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            x, y, w, h = faces[0]  # first face\n",
    "            pad = int(0.15*w)\n",
    "            x1, y1 = max(0,x-pad), max(0,y-pad)\n",
    "            x2, y2 = min(frame.shape[1], x+w+pad), min(frame.shape[0], y+h+pad)\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "            overlay_img, emotion, conf = compute_xai_overlay(face_roi, mode)\n",
    "            window_name = f\"{mode.upper()} Overlay\"\n",
    "            cv2.imshow(window_name, overlay_img)\n",
    "        else:\n",
    "            print(\"No face detected for XAI overlay.\")\n",
    "\n",
    "    # Close overlay windows ---\n",
    "    if key == ord('n'):\n",
    "        for win in [\"GRADCAM Overlay\",\"SMOOTHGRAD Overlay\",\"ACTIVATION Overlay\"]:\n",
    "            cv2.destroyWindow(win)\n",
    "\n",
    "    # Quit\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
